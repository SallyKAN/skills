"""
è®ºæ–‡è§£ææ¨¡å—
- ä¸‹è½½PDF
- æå–æ–‡æœ¬å†…å®¹
- æå–å›¾è¡¨å¹¶æˆªå›¾ä¿å­˜
"""

import re
import tempfile
from dataclasses import dataclass, field
from pathlib import Path
from urllib.parse import urlparse

import fitz  # PyMuPDF
import httpx
from PIL import Image


@dataclass
class Figure:
    """è®ºæ–‡ä¸­çš„å›¾è¡¨"""
    page_num: int
    index: int
    bbox: tuple[float, float, float, float]  # x0, y0, x1, y1
    caption: str = ""
    image_path: Path | None = None


@dataclass
class PaperContent:
    """è§£æåçš„è®ºæ–‡å†…å®¹"""
    title: str
    authors: list[str]
    abstract: str
    sections: dict[str, str]  # section_title -> content
    figures: list[Figure]
    references: list[str]
    full_text: str
    pdf_path: Path
    metadata: dict = field(default_factory=dict)


class PaperParser:
    """è®ºæ–‡è§£æå™¨"""

    def __init__(self, output_dir: Path | str = "./output"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.figures_dir = self.output_dir / "figures"
        self.figures_dir.mkdir(exist_ok=True)

    def parse(self, url: str) -> PaperContent:
        """
        è§£æè®ºæ–‡URLï¼Œè¿”å›ç»“æ„åŒ–å†…å®¹

        æ”¯æŒ:
        - arXivé“¾æ¥ (abs/pdf)
        - ç›´æ¥PDFé“¾æ¥
        """
        pdf_path = self._download_pdf(url)
        return self._parse_pdf(pdf_path)

    def _download_pdf(self, url: str) -> Path:
        """ä¸‹è½½PDFæ–‡ä»¶"""
        # å¤„ç†arXivé“¾æ¥
        if "arxiv.org" in url:
            url = self._normalize_arxiv_url(url)

        print(f"ğŸ“¥ ä¸‹è½½è®ºæ–‡: {url}")

        with httpx.Client(follow_redirects=True, timeout=60.0) as client:
            response = client.get(url)
            response.raise_for_status()

        # ä¿å­˜PDF
        filename = self._extract_filename(url)
        pdf_path = self.output_dir / filename
        pdf_path.write_bytes(response.content)

        print(f"âœ… PDFå·²ä¿å­˜: {pdf_path}")
        return pdf_path

    def _normalize_arxiv_url(self, url: str) -> str:
        """å°†arXivé“¾æ¥è½¬æ¢ä¸ºPDFä¸‹è½½é“¾æ¥"""
        # https://arxiv.org/abs/1706.03762 -> https://arxiv.org/pdf/1706.03762.pdf
        # https://arxiv.org/pdf/1706.03762 -> https://arxiv.org/pdf/1706.03762.pdf

        if "/abs/" in url:
            url = url.replace("/abs/", "/pdf/")

        if not url.endswith(".pdf"):
            url = url + ".pdf"

        return url

    def _extract_filename(self, url: str) -> str:
        """ä»URLæå–æ–‡ä»¶å"""
        parsed = urlparse(url)
        path = parsed.path

        # arXivæ ¼å¼: /pdf/1706.03762.pdf
        if "arxiv.org" in url:
            match = re.search(r"(\d+\.\d+)", path)
            if match:
                return f"arxiv_{match.group(1)}.pdf"

        # å…¶ä»–æƒ…å†µ
        filename = Path(path).name
        if not filename.endswith(".pdf"):
            filename = "paper.pdf"

        return filename

    def _parse_pdf(self, pdf_path: Path) -> PaperContent:
        """è§£æPDFæ–‡ä»¶"""
        print(f"ğŸ“„ è§£æPDF: {pdf_path}")

        doc = fitz.open(pdf_path)

        # æå–å…ƒä¿¡æ¯
        metadata = doc.metadata

        # æå–å…¨æ–‡
        full_text = ""
        for page in doc:
            full_text += page.get_text()

        # æå–ç»“æ„åŒ–å†…å®¹
        title = self._extract_title(doc, metadata)
        authors = self._extract_authors(doc, full_text)
        abstract = self._extract_abstract(full_text)
        sections = self._extract_sections(full_text)
        figures = self._extract_figures(doc, pdf_path)
        references = self._extract_references(full_text)

        doc.close()

        print(f"âœ… è§£æå®Œæˆ: {title}")
        print(f"   - ä½œè€…: {len(authors)}äºº")
        print(f"   - ç« èŠ‚: {len(sections)}ä¸ª")
        print(f"   - å›¾è¡¨: {len(figures)}ä¸ª")

        return PaperContent(
            title=title,
            authors=authors,
            abstract=abstract,
            sections=sections,
            figures=figures,
            references=references,
            full_text=full_text,
            pdf_path=pdf_path,
            metadata=dict(metadata) if metadata else {},
        )

    def _extract_title(self, doc: fitz.Document, metadata: dict) -> str:
        """æå–è®ºæ–‡æ ‡é¢˜"""
        # ä¼˜å…ˆä½¿ç”¨å…ƒæ•°æ®
        if metadata and metadata.get("title"):
            return metadata["title"]

        # ä»ç¬¬ä¸€é¡µæå–ï¼ˆé€šå¸¸æ˜¯æœ€å¤§å­—ä½“çš„æ–‡æœ¬ï¼‰
        first_page = doc[0]
        blocks = first_page.get_text("dict")["blocks"]

        max_size = 0
        title = ""

        for block in blocks:
            if "lines" in block:
                for line in block["lines"]:
                    for span in line["spans"]:
                        if span["size"] > max_size:
                            max_size = span["size"]
                            title = span["text"]

        return title.strip() or "Unknown Title"

    def _extract_authors(self, doc: fitz.Document, full_text: str) -> list[str]:
        """æå–ä½œè€…åˆ—è¡¨"""
        # ç®€å•å®ç°ï¼šä»ç¬¬ä¸€é¡µæå–
        # å®é™…åœºæ™¯å¯èƒ½éœ€è¦æ›´å¤æ‚çš„NERæˆ–è§„åˆ™

        first_page_text = doc[0].get_text()
        lines = first_page_text.split("\n")

        authors = []
        in_author_section = False

        for line in lines[:30]:  # åªçœ‹å‰30è¡Œ
            line = line.strip()

            # è·³è¿‡æ ‡é¢˜ï¼ˆé€šå¸¸æ˜¯å¤§å†™æˆ–å¾ˆé•¿ï¼‰
            if len(line) > 100:
                continue

            # æ£€æµ‹ä½œè€…åŒºåŸŸï¼ˆé€šå¸¸åœ¨æ ‡é¢˜åï¼Œæ‘˜è¦å‰ï¼‰
            if "abstract" in line.lower():
                break

            # ç®€å•çš„äººåæ£€æµ‹ï¼ˆåŒ…å«é€—å·åˆ†éš”çš„åå­—ï¼‰
            if "," in line and len(line) < 200:
                # å¯èƒ½æ˜¯ä½œè€…åˆ—è¡¨
                names = [n.strip() for n in line.split(",")]
                for name in names:
                    # è¿‡æ»¤æ˜æ˜¾ä¸æ˜¯äººåçš„
                    if name and len(name) > 2 and not any(c.isdigit() for c in name):
                        if "@" not in name and "university" not in name.lower():
                            authors.append(name)

        return authors[:20]  # æœ€å¤šè¿”å›20ä¸ªä½œè€…

    def _extract_abstract(self, full_text: str) -> str:
        """æå–æ‘˜è¦"""
        # æŸ¥æ‰¾Abstractéƒ¨åˆ†
        patterns = [
            r"Abstract[\s\n]+(.+?)(?=\n\s*\n|\n1[\.\s]|Introduction)",
            r"ABSTRACT[\s\n]+(.+?)(?=\n\s*\n|\n1[\.\s]|INTRODUCTION)",
        ]

        for pattern in patterns:
            match = re.search(pattern, full_text, re.DOTALL | re.IGNORECASE)
            if match:
                abstract = match.group(1).strip()
                # æ¸…ç†æ¢è¡Œ
                abstract = re.sub(r"\s+", " ", abstract)
                return abstract[:2000]  # é™åˆ¶é•¿åº¦

        return ""

    def _extract_sections(self, full_text: str) -> dict[str, str]:
        """æå–ç« èŠ‚å†…å®¹"""
        sections = {}

        # åŒ¹é…ç« èŠ‚æ ‡é¢˜æ¨¡å¼ï¼š1. Introduction æˆ– 1 Introduction æˆ– ## Introduction
        section_pattern = r"\n(\d+\.?\s+[A-Z][^\n]{3,50})\n"

        matches = list(re.finditer(section_pattern, full_text))

        for i, match in enumerate(matches):
            title = match.group(1).strip()
            start = match.end()
            end = matches[i + 1].start() if i + 1 < len(matches) else len(full_text)

            content = full_text[start:end].strip()
            # æ¸…ç†å†…å®¹
            content = re.sub(r"\s+", " ", content)

            sections[title] = content[:5000]  # é™åˆ¶æ¯ä¸ªç« èŠ‚é•¿åº¦

        return sections

    def _extract_figures(self, doc: fitz.Document, pdf_path: Path) -> list[Figure]:
        """æå–å›¾è¡¨å¹¶ä¿å­˜ä¸ºå›¾ç‰‡"""
        figures = []

        for page_num, page in enumerate(doc):
            # è·å–é¡µé¢ä¸Šçš„å›¾ç‰‡
            image_list = page.get_images()

            for img_index, img in enumerate(image_list):
                xref = img[0]

                try:
                    # æå–å›¾ç‰‡
                    base_image = doc.extract_image(xref)
                    image_bytes = base_image["image"]
                    image_ext = base_image["ext"]

                    # ä¿å­˜å›¾ç‰‡
                    img_filename = f"fig_p{page_num + 1}_{img_index + 1}.{image_ext}"
                    img_path = self.figures_dir / img_filename
                    img_path.write_bytes(image_bytes)

                    # è·å–å›¾ç‰‡åœ¨é¡µé¢ä¸Šçš„ä½ç½®
                    img_rect = page.get_image_rects(xref)
                    bbox = img_rect[0] if img_rect else (0, 0, 0, 0)

                    figure = Figure(
                        page_num=page_num + 1,
                        index=img_index + 1,
                        bbox=tuple(bbox),
                        image_path=img_path,
                    )
                    figures.append(figure)

                except Exception as e:
                    print(f"âš ï¸ æå–å›¾ç‰‡å¤±è´¥ (page {page_num + 1}, img {img_index + 1}): {e}")

        # å°è¯•åŒ¹é…å›¾ç‰‡è¯´æ˜æ–‡å­—
        self._match_figure_captions(doc, figures)

        return figures

    def _match_figure_captions(self, doc: fitz.Document, figures: list[Figure]):
        """åŒ¹é…å›¾ç‰‡è¯´æ˜æ–‡å­—"""
        for figure in figures:
            page = doc[figure.page_num - 1]
            text = page.get_text()

            # æŸ¥æ‰¾Figure Xæˆ–Fig. Xæ ¼å¼çš„è¯´æ˜
            pattern = rf"(?:Figure|Fig\.?)\s*{figure.index}[\.:]\s*([^\n]+)"
            match = re.search(pattern, text, re.IGNORECASE)

            if match:
                figure.caption = match.group(1).strip()

    def _extract_references(self, full_text: str) -> list[str]:
        """æå–å‚è€ƒæ–‡çŒ®"""
        references = []

        # æ‰¾åˆ°Referenceséƒ¨åˆ†
        ref_match = re.search(r"References?\s*\n", full_text, re.IGNORECASE)
        if not ref_match:
            return references

        ref_text = full_text[ref_match.end():]

        # åŒ¹é…ç¼–å·çš„å‚è€ƒæ–‡çŒ®
        ref_pattern = r"\[(\d+)\]\s*([^\[]+)"
        matches = re.findall(ref_pattern, ref_text)

        for num, content in matches[:50]:  # æœ€å¤š50æ¡
            ref = content.strip()
            ref = re.sub(r"\s+", " ", ref)
            references.append(f"[{num}] {ref}")

        return references


# ä¾¿æ·å‡½æ•°
def parse_paper(url: str, output_dir: str = "./output") -> PaperContent:
    """è§£æè®ºæ–‡çš„ä¾¿æ·å‡½æ•°"""
    parser = PaperParser(output_dir=output_dir)
    return parser.parse(url)


if __name__ == "__main__":
    # æµ‹è¯•
    import sys

    if len(sys.argv) > 1:
        url = sys.argv[1]
    else:
        # é»˜è®¤æµ‹è¯•ï¼šAttention Is All You Need
        url = "https://arxiv.org/abs/1706.03762"

    content = parse_paper(url)
    print(f"\n{'='*50}")
    print(f"æ ‡é¢˜: {content.title}")
    print(f"ä½œè€…: {', '.join(content.authors[:5])}...")
    print(f"æ‘˜è¦: {content.abstract[:200]}...")
